{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Legendre Memory Unit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77JYtCSwNJT5",
        "colab_type": "text"
      },
      "source": [
        "# Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n3tJ50X3Jo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a8Dhe5_EN_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-jUo87Cm6Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lecun_uniform(tensor):\n",
        "    fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n",
        "    nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYahiHxsgZil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "cbdd1d1d-6b7b-4318-86d1-7d92f6b3ff91"
      },
      "source": [
        "!pip install nengolib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nengolib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/00/62bbce813d135da3799f4afc26957d942d4ae27085fb0df1bfb57dcf692b/nengolib-0.5.2-py2.py3-none-any.whl (117kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 25.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 40kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 51kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 92kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from nengolib) (1.3.3)\n",
            "Collecting nengo<3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/ce/e314e1176bfbbe6c3b6cf4e8fa0620cafad8f8bad04203c55881e9cb2fb0/nengo-2.8.0-py2.py3-none-any.whl (375kB)\n",
            "\r\u001b[K     |▉                               | 10kB 32.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 38.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 46.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 48.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 49.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 61kB 52.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 54.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 56.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 57.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112kB 57.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 122kB 57.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 133kB 57.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 57.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 57.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163kB 57.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 174kB 57.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 184kB 57.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 194kB 57.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 57.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215kB 57.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 225kB 57.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 57.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 245kB 57.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 256kB 57.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 266kB 57.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 276kB 57.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 286kB 57.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 296kB 57.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 307kB 57.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317kB 57.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 327kB 57.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 337kB 57.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 348kB 57.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 358kB 57.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 368kB 57.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 57.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.6/dist-packages (from nengolib) (1.17.4)\n",
            "Installing collected packages: nengo, nengolib\n",
            "Successfully installed nengo-2.8.0 nengolib-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qis4g0t_NP7T",
        "colab_type": "text"
      },
      "source": [
        "# LMU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAiRtAIA3kM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sympy.matrices import Matrix, eye, zeros, ones, diag, GramSchmidt\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "from nengolib.signal import Identity, cont2discrete\n",
        "from nengolib.synapses import LegendreDelay\n",
        "from functools import partial\n",
        "\n",
        "class LegendreMemoryUnitCell(nn.Module):\n",
        "  def __init__(self, input_dim, units , order, theta,\n",
        "                 input_encoders_initializer=lecun_uniform,\n",
        "                 hidden_encoders_initializer=lecun_uniform,\n",
        "                 memory_encoders_initializer=partial(torch.nn.init.constant_, val=0),\n",
        "                 input_kernel_initializer=torch.nn.init.xavier_normal_,\n",
        "                 hidden_kernel_initializer=torch.nn.init.xavier_normal_,\n",
        "                 memory_kernel_initializer=torch.nn.init.xavier_normal_):\n",
        "    super(LegendreMemoryUnitCell, self).__init__()\n",
        "\n",
        "    self.order = order\n",
        "    self.theta = theta\n",
        "    self.units = units\n",
        "    \n",
        "\n",
        "    realizer = Identity()\n",
        "    self._realizer_result = realizer(LegendreDelay(theta=theta, order=self.order))\n",
        "\n",
        "    self._ss = cont2discrete(self._realizer_result.realization, dt=1., method='zoh')\n",
        "\n",
        "    self._A = self._ss.A - np.eye(order)\n",
        "    self._B = self._ss.B\n",
        "    self._C = self._ss.C\n",
        "\n",
        "    self.AT = nn.Parameter(torch.Tensor(self._A), requires_grad=False)\n",
        "    self.BT = nn.Parameter(torch.Tensor(self._B), requires_grad=False)\n",
        "\n",
        "\n",
        "    self.encoder_input = nn.Parameter(torch.Tensor(1,input_dim), requires_grad=True)\n",
        "    self.encoder_hidden = nn.Parameter(torch.Tensor(1,self.units), requires_grad=True)\n",
        "    self.encoder_memory = nn.Parameter(torch.Tensor(1,self.order ), requires_grad=True)\n",
        "    self.kernel_input = nn.Parameter(torch.Tensor(self.units, input_dim), requires_grad=True)\n",
        "    self.kernel_hidden = nn.Parameter(torch.Tensor(self.units, self.units), requires_grad=True)\n",
        "    self.kernel_memory = nn.Parameter(torch.Tensor(self.units, self.order), requires_grad=True)\n",
        "    \n",
        "\n",
        "    input_encoders_initializer(self.encoder_input)\n",
        "    hidden_encoders_initializer(self.encoder_hidden)\n",
        "    memory_encoders_initializer(self.encoder_memory)\n",
        "    input_kernel_initializer(self.kernel_input)\n",
        "    hidden_kernel_initializer(self.kernel_hidden)\n",
        "    memory_kernel_initializer(self.kernel_memory)\n",
        "\n",
        "  def EulerOdeSolver(self):\n",
        "    A_hat = (self.step_delta_t/self.theta)*self.AT + torch.eye(self.order,self.d_order_ode)\n",
        "    B_hat = (self.step_delta_t/self.theta)*self.BT\n",
        "\n",
        "    return A_hat, B_hat\n",
        "\n",
        "  def forward(self, xt, states):\n",
        "    ht, mt = states\n",
        "\n",
        "    ut = F.linear(xt, self.encoder_input) + F.linear(ht, self.encoder_hidden) + F.linear(mt, self.encoder_memory)\n",
        "\n",
        "    mt = mt + F.linear(mt, self.AT) + F.linear(ut, self.BT)\n",
        "\n",
        "    ht = nn.Tanh()(F.linear(xt, self.kernel_input) + F.linear(ht, self.kernel_hidden) + F.linear(mt, self.kernel_memory))\n",
        "    \n",
        "    return ht, (ht, mt)\n",
        "\n",
        "class LegendreMemoryUnit(nn.Module):\n",
        "  def __init__(self, input_dim, units , order, theta):\n",
        "    super(LegendreMemoryUnit, self).__init__()\n",
        "\n",
        "    self.units = units\n",
        "    self.order = order\n",
        "\n",
        "    self.lmucell = LegendreMemoryUnitCell(input_dim, units , order, theta)\n",
        "\n",
        "  def forward(self, xt):\n",
        "    '''\n",
        "      batch first :\n",
        "        xt shape : (batch, seq_len, nb_feature)\n",
        "    '''\n",
        "    outputs = []\n",
        "    \n",
        "    h0 = torch.zeros(xt.size(0),self.units).cuda()\n",
        "    m0 = torch.zeros(xt.size(0),self.order).cuda()\n",
        "    states = (h0,m0)\n",
        "    for i in range(xt.size(1)):\n",
        "      out, states = self.lmucell(xt[:,i,:], states)\n",
        "      outputs += [out]\n",
        "    return torch.stack(outputs).permute(1,0,2), states\n",
        "    # return torch.stack(outputs).permute(1,0,2), states\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRC62TebNYWJ",
        "colab_type": "text"
      },
      "source": [
        "# Test LMU output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfTlZrsfbSXH",
        "colab_type": "code",
        "outputId": "1c7e40ce-2c86-4db2-f46e-b988874fab23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.rand(64,5000,1)\n",
        "h0 = torch.rand(64,49)\n",
        "m0 = torch.rand(64,4)\n",
        "\n",
        "# # print(\"Parameters : \", len(model.parameters()))\n",
        "# # print(\"e\")\n",
        "# nb_params = 0\n",
        "# for parameter in model.parameters():\n",
        "#     # print(parameter.shape)\n",
        "#     nb_params += parameter.shape[0]*parameter.shape[0]\n",
        "\n",
        "# print(nb_params)\n",
        "model = LegendreMemoryUnit(1,49,4,4)\n",
        "res, _ = model(x)\n",
        "# # print(\"d\")\n",
        "print(res.shape)\n",
        "# # print(res[0].shape, res[1][0].shape, res[1][1].shape)\n",
        "\n",
        "# # summary(model, (5000,1))\n",
        "\n",
        "# lstm = nn.LSTM(1,49, batch_first=True)\n",
        "# a, states = lstm(x)\n",
        "# # print(a.shape)\n",
        "# # print(states[0].shape)\n",
        "# # print(states[1].shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 5000, 49])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRuoDB59NvMX",
        "colab_type": "text"
      },
      "source": [
        "# Models LSTM - LMU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51nVz6TsX9Fc",
        "colab_type": "code",
        "outputId": "53f128f0-009c-4f44-e03e-387b3bfb6ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "class LMUModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LMUModel, self).__init__()\n",
        "    self.LMU = LegendreMemoryUnit(1,49,4,4)\n",
        "    self.dense = nn.Linear(49,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x, _ = self.LMU(x)\n",
        "    x = self.dense(x)\n",
        "\n",
        "    return x\n",
        "x = torch.rand(32,5000,1)\n",
        "model_lstm = LMUModel()\n",
        "print(model_lstm(x).shape)\n",
        "print(\"Nombre de paramètres : \", sum(p.numel() for p in model_lstm.parameters() if p.requires_grad))\n",
        "print()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 5000, 1])\n",
            "Nombre de paramètres :  2750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrNF6LetOGun",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f1fcfa84-d9aa-4426-8101-cf0f90a76067"
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LSTMModel, self).__init__()\n",
        "    self.LSTM = nn.LSTM(1,25,1,batch_first=True)\n",
        "    self.dense = nn.Linear(25,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x, _ = self.LSTM(x)\n",
        "    x = self.dense(x)\n",
        "\n",
        "    return x\n",
        "x = torch.rand(32,5000,1)\n",
        "model_lstm = LSTMModel()\n",
        "print(model_lstm(x).shape)\n",
        "print(\"Nombre de paramètres : \", sum(p.numel() for p in model_lstm.parameters() if p.requires_grad))\n",
        "print()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 5000, 1])\n",
            "Nombre de paramètres :  2826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soOI6lS9Ocas",
        "colab_type": "text"
      },
      "source": [
        "# MackeyGlass Datas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voyo-rQYD-56",
        "colab_type": "code",
        "outputId": "aa06fee7-5f82-4c0d-ef0c-a18dabc7d877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "import collections \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.recurrent import LSTM, RNN\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torchsummary import summary\n",
        "\n",
        "def mackey_glass(sample_len=1000, tau=17, delta_t=10, seed=None, n_samples=1):\n",
        "    # Adapted from https://github.com/mila-iqia/summerschool2015/blob/master/rnn_tutorial/synthetic.py\n",
        "    '''\n",
        "    mackey_glass(sample_len=1000, tau=17, seed = None, n_samples = 1) -> input\n",
        "    Generate the Mackey Glass time-series. Parameters are:\n",
        "        - sample_len: length of the time-series in timesteps. Default is 1000.\n",
        "        - tau: delay of the MG - system. Commonly used values are tau=17 (mild \n",
        "          chaos) and tau=30 (moderate chaos). Default is 17.\n",
        "        - seed: to seed the random generator, can be used to generate the same\n",
        "          timeseries at each invocation.\n",
        "        - n_samples : number of samples to generate\n",
        "    '''\n",
        "    history_len = tau * delta_t \n",
        "    # Initial conditions for the history of the system\n",
        "    timeseries = 1.2\n",
        "    \n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        history = collections.deque(1.2 * np.ones(history_len) + 0.2 * \\\n",
        "                                    (np.random.rand(history_len) - 0.5))\n",
        "        # Preallocate the array for the time-series\n",
        "        inp = np.zeros((sample_len,1))\n",
        "        \n",
        "        for timestep in range(sample_len):\n",
        "            for _ in range(delta_t):\n",
        "                xtau = history.popleft()\n",
        "                history.append(timeseries)\n",
        "                timeseries = history[-1] + (0.2 * xtau / (1.0 + xtau ** 10) - \\\n",
        "                             0.1 * history[-1]) / delta_t\n",
        "            inp[timestep] = timeseries\n",
        "        \n",
        "        # Squash timeseries through tanh\n",
        "        inp = np.tanh(inp - 1)\n",
        "        samples.append(inp)\n",
        "    return samples\n",
        "\n",
        "def generate_data(n_batches, length, split=0.5, seed=0,\n",
        "                  predict_length=15, tau=17, washout=100, delta_t=1,\n",
        "                  center=True):\n",
        "    X = np.asarray(mackey_glass(\n",
        "        sample_len=length+predict_length+washout, tau=tau,\n",
        "        seed=seed, n_samples=n_batches))\n",
        "    X = X[:, washout:, :]\n",
        "    cutoff = int(split*n_batches)\n",
        "    if center:\n",
        "        X -= np.mean(X)  # global mean over all batches, approx -0.066\n",
        "    Y = X[:, :-predict_length, :]\n",
        "    X = X[:, predict_length:, :]\n",
        "    assert X.shape == Y.shape\n",
        "    return ((X[:cutoff], Y[:cutoff]),\n",
        "            (X[cutoff:], Y[cutoff:]))\n",
        "\n",
        "(train_X, train_Y), (test_X, test_Y) = generate_data(128, 5000)\n",
        "print(train_X.shape, test_X.shape)\n",
        "\n",
        "def cool_plot(X, Y, title=\"\"):\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    plt.title(title)\n",
        "    plt.scatter(X[:, 0], Y[:, 0] - X[:, 0], s=8, alpha=0.7,\n",
        "                c=np.arange(X.shape[0]), cmap=sns.cubehelix_palette(as_cmap=True))\n",
        "    plt.plot(X[:, 0], Y[:, 0] - X[:, 0], c='black', alpha=0.2)\n",
        "    plt.xlabel(\"$x(t)$\")\n",
        "    plt.ylabel(\"$y(t) - x(t)$\")\n",
        "    sns.despine(offset=15)\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# cool_plot(train_X[0], train_Y[0])\n",
        "\n",
        "plt.plot(train_X[0][0:100])\n",
        "plt.plot(train_Y[0][0:100])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(64, 5000, 1) (64, 5000, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xncoH8HPLsg",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaCT4Pi_PakY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, epochs, dataset, dataset_valid = None):\n",
        "  for e in range(epochs):\n",
        "      model.train()\n",
        "      running_loss = 0\n",
        "      with tqdm(total=len(dataset)) as bar:\n",
        "        for i, (X, y) in enumerate(dataset):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model(X.cuda())\n",
        "            # print(X.shape)\n",
        "\n",
        "            loss_ll = criterion(output, y.cuda())\n",
        "            # print()\n",
        "            # print(output[0,0:10,0], y[0,0:10,0])\n",
        "            \n",
        "            loss_ll.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss_ll.item()\n",
        "\n",
        "            bar.update(1)\n",
        "            bar.set_description(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(dataset)))\n",
        "\n",
        "      model.eval()\n",
        "      running_loss = 0\n",
        "      outs = []\n",
        "      with tqdm(total=len(dataset_valid)) as bar:\n",
        "        for i, (X, y) in enumerate(dataset_valid):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model(X.cuda())\n",
        "            # print(X.shape)\n",
        "\n",
        "            loss_ll = criterion(output, y.cuda())\n",
        "            # print()\n",
        "            # print(output.shape)\n",
        "                \n",
        "            running_loss += loss_ll.item()\n",
        "            \n",
        "            outs.append(output)\n",
        "            bar.update(1)\n",
        "            bar.set_description(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(dataset_valid)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEwfu8TUPTnI",
        "colab_type": "text"
      },
      "source": [
        "# Test LMU vs LSTM on MackeyGlass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUlSXcORWqtn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GIa1QmHpIr1",
        "colab_type": "code",
        "outputId": "a5cbe30e-9193-4464-d44d-3d2955f74d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "\n",
        "model = LMUModel().cuda()\n",
        "print(\"\\n\\nNombre de paramètres : \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"\\n\\n\")\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=1e-1)\n",
        "model.train()\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(torch.Tensor(train_X).cuda(), torch.Tensor(train_Y).cuda())\n",
        "dataset = data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "dataset_valid = torch.utils.data.TensorDataset(torch.Tensor(test_X).cuda(), torch.Tensor(test_Y).cuda())\n",
        "dataset_valid = data.DataLoader(dataset_valid, batch_size=16, shuffle=False)\n",
        "\n",
        "train(model, 100, dataset, dataset_valid)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Nombre de paramètres :  2750\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - Training loss: 0.11691330000758171: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it]\n",
            "Epoch 0 - Training loss: 0.08149615675210953: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it] \n",
            "Epoch 1 - Training loss: 0.07011827453970909: 100%|██████████| 4/4 [00:13<00:00,  3.24s/it]\n",
            "Epoch 1 - Training loss: 0.07616016641259193: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]\n",
            "Epoch 2 - Training loss: 0.0619095778092742: 100%|██████████| 4/4 [00:12<00:00,  3.10s/it]  \n",
            "Epoch 2 - Training loss: 0.04228962119668722: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 3 - Training loss: 0.03691740334033966: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it] \n",
            "Epoch 3 - Training loss: 0.03011553082615137: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it] \n",
            "Epoch 4 - Training loss: 0.027084638364613056: 100%|██████████| 4/4 [00:12<00:00,  3.09s/it]\n",
            "Epoch 4 - Training loss: 0.022634524386376143: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 5 - Training loss: 0.020509517285972834: 100%|██████████| 4/4 [00:12<00:00,  3.14s/it]\n",
            "Epoch 5 - Training loss: 0.01737078744918108: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it] \n",
            "Epoch 6 - Training loss: 0.01584360795095563: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it]\n",
            "Epoch 6 - Training loss: 0.013571318238973618: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 7 - Training loss: 0.012454679934307933: 100%|██████████| 4/4 [00:12<00:00,  3.20s/it]\n",
            "Epoch 7 - Training loss: 0.010779951000586152: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 8 - Training loss: 0.00994650018401444: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it] \n",
            "Epoch 8 - Training loss: 0.00868089054711163: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it] \n",
            "Epoch 9 - Training loss: 0.008042395580559969: 100%|██████████| 4/4 [00:12<00:00,  3.16s/it]\n",
            "Epoch 9 - Training loss: 0.007060816744342446: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it] \n",
            "Epoch 10 - Training loss: 0.006564534502103925: 100%|██████████| 4/4 [00:12<00:00,  3.13s/it]\n",
            "Epoch 10 - Training loss: 0.005798166966997087: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n",
            "Epoch 11 - Training loss: 0.0054173595272004604: 100%|██████████| 4/4 [00:12<00:00,  3.16s/it]\n",
            "Epoch 11 - Training loss: 0.004829422803595662: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it] \n",
            "Epoch 12 - Training loss: 0.004544148105196655: 100%|██████████| 4/4 [00:12<00:00,  3.12s/it] \n",
            "Epoch 12 - Training loss: 0.0041005032835528255: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 13 - Training loss: 0.003887054685037583: 100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n",
            "Epoch 13 - Training loss: 0.0035484821419231594: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]\n",
            "Epoch 14 - Training loss: 0.003385148593224585: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it]\n",
            "Epoch 14 - Training loss: 0.0031188270077109337: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 15 - Training loss: 0.0029907384887337685: 100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n",
            "Epoch 15 - Training loss: 0.0027768195723183453: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 16 - Training loss: 0.0026769558317027986: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it]\n",
            "Epoch 16 - Training loss: 0.002506253367755562: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it] \n",
            "Epoch 17 - Training loss: 0.0024309001164510846: 100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n",
            "Epoch 17 - Training loss: 0.0022969134151935577: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 18 - Training loss: 0.0022407618816941977: 100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n",
            "Epoch 18 - Training loss: 0.002132990804966539: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it] \n",
            "Epoch 19 - Training loss: 0.0020884391269646585: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it]\n",
            "Epoch 19 - Training loss: 0.001994653604924679: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it] \n",
            "Epoch 20 - Training loss: 0.001955385581823066: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it] \n",
            "Epoch 20 - Training loss: 0.0018683955422602594: 100%|██████████| 4/4 [00:05<00:00,  1.51s/it]\n",
            "Epoch 21 - Training loss: 0.0018351447070017457: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it]\n",
            "Epoch 21 - Training loss: 0.0017753128777258098: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 22 - Training loss: 0.0018452710646670312: 100%|██████████| 4/4 [00:12<00:00,  3.25s/it]\n",
            "Epoch 22 - Training loss: 0.0020024930126965046: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n",
            "Epoch 23 - Training loss: 0.002051313000265509: 100%|██████████| 4/4 [00:12<00:00,  3.16s/it]\n",
            "Epoch 23 - Training loss: 0.002086869382765144: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it] \n",
            "Epoch 24 - Training loss: 0.0020913183107040823: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it]\n",
            "Epoch 24 - Training loss: 0.0020798497716896236: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]\n",
            "Epoch 25 - Training loss: 0.0020542590063996613: 100%|██████████| 4/4 [00:13<00:00,  3.23s/it]\n",
            "Epoch 25 - Training loss: 0.0020177289261482656: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 26 - Training loss: 0.0019930315029341727: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it]\n",
            "Epoch 26 - Training loss: 0.001948463701410219: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it] \n",
            "Epoch 27 - Training loss: 0.001924232259625569: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it] \n",
            "Epoch 27 - Training loss: 0.0018901127623394132: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 28 - Training loss: 0.0018505316402297467: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it]\n",
            "Epoch 28 - Training loss: 0.0018080139416269958: 100%|██████████| 4/4 [00:06<00:00,  1.50s/it]\n",
            "Epoch 29 - Training loss: 0.0017966126033570617: 100%|██████████| 4/4 [00:12<00:00,  3.20s/it]\n",
            "Epoch 29 - Training loss: 0.0017761429480742663: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 30 - Training loss: 0.0017568410257808864: 100%|██████████| 4/4 [00:13<00:00,  3.24s/it]\n",
            "Epoch 30 - Training loss: 0.001723098015645519: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 31 - Training loss: 0.0016948737611528486: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it]\n",
            "Epoch 31 - Training loss: 0.0016632609476801008: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 32 - Training loss: 0.0016445544897578657: 100%|██████████| 4/4 [00:12<00:00,  3.21s/it]\n",
            "Epoch 32 - Training loss: 0.0016083355876617134: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 33 - Training loss: 0.001609585917321965: 100%|██████████| 4/4 [00:12<00:00,  3.25s/it] \n",
            "Epoch 33 - Training loss: 0.001591499021742493: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it] \n",
            "Epoch 34 - Training loss: 0.001598304050276056: 100%|██████████| 4/4 [00:12<00:00,  3.20s/it] \n",
            "Epoch 34 - Training loss: 0.0015716174966655672: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 35 - Training loss: 0.0015498403809033334: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it]\n",
            "Epoch 35 - Training loss: 0.0015122084296308458: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 36 - Training loss: 0.001516509335488081: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it] \n",
            "Epoch 36 - Training loss: 0.0014873090549372137: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 37 - Training loss: 0.0014954530342947692: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it]\n",
            "Epoch 37 - Training loss: 0.0014849048166070133: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 38 - Training loss: 0.0014667918148916215: 100%|██████████| 4/4 [00:13<00:00,  3.24s/it]\n",
            "Epoch 38 - Training loss: 0.001432264834875241: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it] \n",
            "Epoch 39 - Training loss: 0.001442793436581269: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it] \n",
            "Epoch 39 - Training loss: 0.0014272606058511883: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 40 - Training loss: 0.0014416420308407396: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it]\n",
            "Epoch 40 - Training loss: 0.001416375074768439: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it] \n",
            "Epoch 41 - Training loss: 0.0014172811061143875: 100%|██████████| 4/4 [00:13<00:00,  3.29s/it]\n",
            "Epoch 41 - Training loss: 0.001391608384437859: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it] \n",
            "Epoch 42 - Training loss: 0.0014078501262702048: 100%|██████████| 4/4 [00:12<00:00,  3.20s/it]\n",
            "Epoch 42 - Training loss: 0.001371398422634229: 100%|██████████| 4/4 [00:06<00:00,  1.50s/it] \n",
            "Epoch 43 - Training loss: 0.0013825678906869143: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it]\n",
            "Epoch 43 - Training loss: 0.0013736735563725233: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 44 - Training loss: 0.0013615122879855335: 100%|██████████| 4/4 [00:12<00:00,  3.24s/it]\n",
            "Epoch 44 - Training loss: 0.001342708128504455: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it] \n",
            "Epoch 45 - Training loss: 0.0013603483675979078: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it]\n",
            "Epoch 45 - Training loss: 0.0013489279663190246: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n",
            "Epoch 46 - Training loss: 0.0013393030094448477: 100%|██████████| 4/4 [00:13<00:00,  3.27s/it]\n",
            "Epoch 46 - Training loss: 0.001314331020694226: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it] \n",
            "Epoch 47 - Training loss: 0.0013191779144108295: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it]\n",
            "Epoch 47 - Training loss: 0.0013162571121938527: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 48 - Training loss: 0.0013128143909852952: 100%|██████████| 4/4 [00:12<00:00,  3.25s/it]\n",
            "Epoch 48 - Training loss: 0.0012992291594855487: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 49 - Training loss: 0.001314734690822661: 100%|██████████| 4/4 [00:13<00:00,  3.24s/it] \n",
            "Epoch 49 - Training loss: 0.0012972904660273343: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 50 - Training loss: 0.0012945738853886724: 100%|██████████| 4/4 [00:13<00:00,  3.25s/it]\n",
            "Epoch 50 - Training loss: 0.0012767859734594822: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 51 - Training loss: 0.0012911974918097258: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it]\n",
            "Epoch 51 - Training loss: 0.0012793072091881186: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 52 - Training loss: 0.0012941198656335473: 100%|██████████| 4/4 [00:12<00:00,  3.21s/it]\n",
            "Epoch 52 - Training loss: 0.001271051965886727: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 53 - Training loss: 0.0012712869502138346: 100%|██████████| 4/4 [00:12<00:00,  3.21s/it]\n",
            "Epoch 53 - Training loss: 0.0012429057387635112: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 54 - Training loss: 0.0012340866669546813: 100%|██████████| 4/4 [00:13<00:00,  3.30s/it]\n",
            "Epoch 54 - Training loss: 0.0012132690462749451: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 55 - Training loss: 0.0012294978369027376: 100%|██████████| 4/4 [00:13<00:00,  3.26s/it]\n",
            "Epoch 55 - Training loss: 0.0012301166134420782: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 56 - Training loss: 0.0012471064110286534: 100%|██████████| 4/4 [00:13<00:00,  3.29s/it]\n",
            "Epoch 56 - Training loss: 0.001229156128829345: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it] \n",
            "Epoch 57 - Training loss: 0.0012361685803625733: 100%|██████████| 4/4 [00:13<00:00,  3.31s/it]\n",
            "Epoch 57 - Training loss: 0.0012181999627500772: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n",
            "Epoch 58 - Training loss: 0.0012242053635418415: 100%|██████████| 4/4 [00:13<00:00,  3.35s/it]\n",
            "Epoch 58 - Training loss: 0.001208635076181963: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it] \n",
            "Epoch 59 - Training loss: 0.0012118533486500382: 100%|██████████| 4/4 [00:13<00:00,  3.28s/it]\n",
            "Epoch 59 - Training loss: 0.0011932160123251379: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 60 - Training loss: 0.0011904810089617968: 100%|██████████| 4/4 [00:13<00:00,  3.30s/it]\n",
            "Epoch 60 - Training loss: 0.0011665080673992634: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 61 - Training loss: 0.001196986937429756: 100%|██████████| 4/4 [00:13<00:00,  3.28s/it] \n",
            "Epoch 61 - Training loss: 0.001208097964990884: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it] \n",
            "Epoch 62 - Training loss: 0.0012277171190362424: 100%|██████████| 4/4 [00:13<00:00,  3.27s/it]\n",
            "Epoch 62 - Training loss: 0.0012085644993931055: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n",
            "Epoch 63 - Training loss: 0.0011916470248252153: 100%|██████████| 4/4 [00:13<00:00,  3.28s/it]\n",
            "Epoch 63 - Training loss: 0.0011590119684115052: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 64 - Training loss: 0.0011597126722335815: 100%|██████████| 4/4 [00:13<00:00,  3.26s/it]\n",
            "Epoch 64 - Training loss: 0.0011464700801298022: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n",
            "Epoch 65 - Training loss: 0.0011555438104551286: 100%|██████████| 4/4 [00:12<00:00,  3.21s/it]\n",
            "Epoch 65 - Training loss: 0.0011495576181914657: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 66 - Training loss: 0.001183843269245699: 100%|██████████| 4/4 [00:13<00:00,  3.29s/it] \n",
            "Epoch 66 - Training loss: 0.0011515873775351793: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 67 - Training loss: 0.001145556045230478: 100%|██████████| 4/4 [00:12<00:00,  3.18s/it] \n",
            "Epoch 67 - Training loss: 0.0011216890416108072: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n",
            "Epoch 68 - Training loss: 0.0011422265961300582: 100%|██████████| 4/4 [00:12<00:00,  3.21s/it]\n",
            "Epoch 68 - Training loss: 0.0011355145543348044: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 69 - Training loss: 0.001143445260822773: 100%|██████████| 4/4 [00:12<00:00,  3.25s/it] \n",
            "Epoch 69 - Training loss: 0.0011316746822558343: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]\n",
            "Epoch 70 - Training loss: 0.0011378635535947978: 100%|██████████| 4/4 [00:13<00:00,  3.28s/it]\n",
            "Epoch 70 - Training loss: 0.0011076013615820557: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 71 - Training loss: 0.0011307504610158503: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it]\n",
            "Epoch 71 - Training loss: 0.001117268722737208: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it] \n",
            "Epoch 72 - Training loss: 0.001121692475862801: 100%|██████████| 4/4 [00:13<00:00,  3.28s/it] \n",
            "Epoch 72 - Training loss: 0.0011125201708637178: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 73 - Training loss: 0.001123252761317417: 100%|██████████| 4/4 [00:13<00:00,  3.29s/it] \n",
            "Epoch 73 - Training loss: 0.0011171500955242664: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]\n",
            "Epoch 74 - Training loss: 0.0011169718636665493: 100%|██████████| 4/4 [00:13<00:00,  3.33s/it]\n",
            "Epoch 74 - Training loss: 0.0010978731734212488: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 75 - Training loss: 0.0010995427146553993: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it]\n",
            "Epoch 75 - Training loss: 0.0010980684019159526: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]\n",
            "Epoch 76 - Training loss: 0.0011089009640272707: 100%|██████████| 4/4 [00:12<00:00,  3.17s/it]\n",
            "Epoch 76 - Training loss: 0.001070771919330582: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it] \n",
            "Epoch 77 - Training loss: 0.0010967788111884147: 100%|██████████| 4/4 [00:12<00:00,  3.20s/it]\n",
            "Epoch 77 - Training loss: 0.001078119152225554: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it] \n",
            "Epoch 78 - Training loss: 0.0010944020468741655: 100%|██████████| 4/4 [00:12<00:00,  3.21s/it]\n",
            "Epoch 78 - Training loss: 0.0010706332977861166: 100%|██████████| 4/4 [00:05<00:00,  1.51s/it]\n",
            "Epoch 79 - Training loss: 0.001067531091393903: 100%|██████████| 4/4 [00:12<00:00,  3.24s/it] \n",
            "Epoch 79 - Training loss: 0.0010310915822628886: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]\n",
            "Epoch 80 - Training loss: 0.0010705990134738386: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it]\n",
            "Epoch 80 - Training loss: 0.0010677347599994391: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 81 - Training loss: 0.001094405073672533: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it] \n",
            "Epoch 81 - Training loss: 0.0010716526594478637: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 82 - Training loss: 0.00106777623295784: 100%|██████████| 4/4 [00:13<00:00,  3.30s/it]  \n",
            "Epoch 82 - Training loss: 0.0010634750360623002: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]\n",
            "Epoch 83 - Training loss: 0.0010691074712667614: 100%|██████████| 4/4 [00:13<00:00,  3.25s/it]\n",
            "Epoch 83 - Training loss: 0.0010414465214125812: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 84 - Training loss: 0.0010470374836586416: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it]\n",
            "Epoch 84 - Training loss: 0.0010319744178559631: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 85 - Training loss: 0.001040840958012268: 100%|██████████| 4/4 [00:12<00:00,  3.25s/it] \n",
            "Epoch 85 - Training loss: 0.0010251766652800143: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 86 - Training loss: 0.0010444395302329212: 100%|██████████| 4/4 [00:13<00:00,  3.25s/it]\n",
            "Epoch 86 - Training loss: 0.0010312864906154573: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 87 - Training loss: 0.0010413730924483389: 100%|██████████| 4/4 [00:13<00:00,  3.26s/it]\n",
            "Epoch 87 - Training loss: 0.0010015005827881396: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 88 - Training loss: 0.001033202512189746: 100%|██████████| 4/4 [00:12<00:00,  3.22s/it] \n",
            "Epoch 88 - Training loss: 0.001039905269863084: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it] \n",
            "Epoch 89 - Training loss: 0.001050173188559711: 100%|██████████| 4/4 [00:12<00:00,  3.23s/it] \n",
            "Epoch 89 - Training loss: 0.0010408333037048578: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n",
            "Epoch 90 - Training loss: 0.001041143957991153: 100%|██████████| 4/4 [00:13<00:00,  3.30s/it] \n",
            "Epoch 90 - Training loss: 0.0010359320149291307: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 91 - Training loss: 0.001031594380037859: 100%|██████████| 4/4 [00:13<00:00,  3.28s/it]\n",
            "Epoch 91 - Training loss: 0.0009868725901469588: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]\n",
            "Epoch 92 - Training loss: 0.0009997888992074877: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it]\n",
            "Epoch 92 - Training loss: 0.0009715583873912692: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 93 - Training loss: 0.0009952274558600038: 100%|██████████| 4/4 [00:12<00:00,  3.24s/it]\n",
            "Epoch 93 - Training loss: 0.000984391532256268: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it] \n",
            "Epoch 94 - Training loss: 0.0010124848631676286: 100%|██████████| 4/4 [00:13<00:00,  3.29s/it]\n",
            "Epoch 94 - Training loss: 0.0010015915904659778: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 95 - Training loss: 0.0010115572076756507: 100%|██████████| 4/4 [00:13<00:00,  3.27s/it]\n",
            "Epoch 95 - Training loss: 0.0009737546643009409: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 96 - Training loss: 0.0009950835083145648: 100%|██████████| 4/4 [00:13<00:00,  3.26s/it]\n",
            "Epoch 96 - Training loss: 0.0009821843996178359: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 97 - Training loss: 0.0009861487051239237: 100%|██████████| 4/4 [00:13<00:00,  3.27s/it]\n",
            "Epoch 97 - Training loss: 0.0009627910912968218: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 98 - Training loss: 0.000980337877990678: 100%|██████████| 4/4 [00:12<00:00,  3.19s/it] \n",
            "Epoch 98 - Training loss: 0.0009601689380360767: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 99 - Training loss: 0.0009834330558078364: 100%|██████████| 4/4 [00:12<00:00,  3.20s/it]\n",
            "Epoch 99 - Training loss: 0.0009624200174584985: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaunc4vo9Orw",
        "colab_type": "code",
        "outputId": "68abf283-9280-47ff-bf88-7fb7b6ed4427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "\n",
        "model = LSTMModel().cuda()\n",
        "print(\"\\n\\nNombre de paramètres : \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"\\n\\n\")\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=1e-1)\n",
        "model.train()\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(torch.Tensor(train_X).cuda(), torch.Tensor(train_Y).cuda())\n",
        "dataset = data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "dataset_valid = torch.utils.data.TensorDataset(torch.Tensor(test_X).cuda(), torch.Tensor(test_Y).cuda())\n",
        "dataset_valid = data.DataLoader(dataset_valid, batch_size=16, shuffle=False)\n",
        "\n",
        "train(model, 100, dataset, dataset_valid)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - Training loss: 0.018374154344201088:  25%|██▌       | 1/4 [00:00<00:00,  7.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Nombre de paramètres :  2826\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - Training loss: 0.07263212651014328: 100%|██████████| 4/4 [00:00<00:00,  7.33it/s]\n",
            "Epoch 0 - Training loss: 0.07126307301223278: 100%|██████████| 4/4 [00:00<00:00, 16.29it/s] \n",
            "Epoch 1 - Training loss: 0.07038172520697117: 100%|██████████| 4/4 [00:00<00:00,  7.80it/s]\n",
            "Epoch 1 - Training loss: 0.06906825304031372: 100%|██████████| 4/4 [00:00<00:00, 17.35it/s]\n",
            "Epoch 2 - Training loss: 0.06823867931962013: 100%|██████████| 4/4 [00:00<00:00,  7.64it/s]\n",
            "Epoch 2 - Training loss: 0.06700328178703785: 100%|██████████| 4/4 [00:00<00:00, 15.11it/s] \n",
            "Epoch 3 - Training loss: 0.06622900255024433: 100%|██████████| 4/4 [00:00<00:00,  7.74it/s]\n",
            "Epoch 3 - Training loss: 0.06507469527423382: 100%|██████████| 4/4 [00:00<00:00, 17.55it/s]\n",
            "Epoch 4 - Training loss: 0.0643550343811512: 100%|██████████| 4/4 [00:00<00:00,  7.57it/s] \n",
            "Epoch 4 - Training loss: 0.06328009255230427: 100%|██████████| 4/4 [00:00<00:00, 16.26it/s]\n",
            "Epoch 5 - Training loss: 0.06261319760233164: 100%|██████████| 4/4 [00:00<00:00,  7.49it/s] \n",
            "Epoch 5 - Training loss: 0.061614351347088814: 100%|██████████| 4/4 [00:00<00:00, 17.45it/s]\n",
            "Epoch 6 - Training loss: 0.06099801603704691: 100%|██████████| 4/4 [00:00<00:00,  7.79it/s]\n",
            "Epoch 6 - Training loss: 0.060071567073464394: 100%|██████████| 4/4 [00:00<00:00, 16.74it/s]\n",
            "Epoch 7 - Training loss: 0.059503317810595036: 100%|██████████| 4/4 [00:00<00:00,  7.85it/s]\n",
            "Epoch 7 - Training loss: 0.058645643293857574: 100%|██████████| 4/4 [00:00<00:00, 15.35it/s]\n",
            "Epoch 8 - Training loss: 0.058123182505369186: 100%|██████████| 4/4 [00:00<00:00,  7.82it/s]\n",
            "Epoch 8 - Training loss: 0.05733078345656395: 100%|██████████| 4/4 [00:00<00:00, 16.82it/s]\n",
            "Epoch 9 - Training loss: 0.05685195792466402: 100%|██████████| 4/4 [00:00<00:00,  7.68it/s] \n",
            "Epoch 9 - Training loss: 0.05612150579690933: 100%|██████████| 4/4 [00:00<00:00, 17.25it/s] \n",
            "Epoch 10 - Training loss: 0.05568428710103035: 100%|██████████| 4/4 [00:00<00:00,  7.37it/s]\n",
            "Epoch 10 - Training loss: 0.05501263588666916: 100%|██████████| 4/4 [00:00<00:00, 15.26it/s]\n",
            "Epoch 11 - Training loss: 0.054614986293017864: 100%|██████████| 4/4 [00:00<00:00,  7.80it/s]\n",
            "Epoch 11 - Training loss: 0.0539991669356823: 100%|██████████| 4/4 [00:00<00:00, 15.45it/s]  \n",
            "Epoch 12 - Training loss: 0.05363924987614155: 100%|██████████| 4/4 [00:00<00:00,  7.64it/s] \n",
            "Epoch 12 - Training loss: 0.05307636596262455: 100%|██████████| 4/4 [00:00<00:00, 16.22it/s]\n",
            "Epoch 13 - Training loss: 0.052752312272787094: 100%|██████████| 4/4 [00:00<00:00,  7.63it/s]\n",
            "Epoch 13 - Training loss: 0.052239544689655304: 100%|██████████| 4/4 [00:00<00:00, 16.72it/s]\n",
            "Epoch 14 - Training loss: 0.05194949824362993: 100%|██████████| 4/4 [00:00<00:00,  7.66it/s]\n",
            "Epoch 14 - Training loss: 0.0514840679243207: 100%|██████████| 4/4 [00:00<00:00, 16.05it/s] \n",
            "Epoch 15 - Training loss: 0.051226272247731686: 100%|██████████| 4/4 [00:00<00:00,  7.75it/s]\n",
            "Epoch 15 - Training loss: 0.050805384293198586: 100%|██████████| 4/4 [00:00<00:00, 15.87it/s]\n",
            "Epoch 16 - Training loss: 0.050577918998897076: 100%|██████████| 4/4 [00:00<00:00,  7.71it/s]\n",
            "Epoch 16 - Training loss: 0.05019879434257746: 100%|██████████| 4/4 [00:00<00:00, 17.10it/s]\n",
            "Epoch 17 - Training loss: 0.04999982099980116: 100%|██████████| 4/4 [00:00<00:00,  7.71it/s]\n",
            "Epoch 17 - Training loss: 0.049659631215035915: 100%|██████████| 4/4 [00:00<00:00, 16.82it/s]\n",
            "Epoch 18 - Training loss: 0.04948732256889343: 100%|██████████| 4/4 [00:00<00:00,  7.82it/s]\n",
            "Epoch 18 - Training loss: 0.04918323550373316: 100%|██████████| 4/4 [00:00<00:00, 15.16it/s]\n",
            "Epoch 19 - Training loss: 0.04903569631278515: 100%|██████████| 4/4 [00:00<00:00,  7.50it/s] \n",
            "Epoch 19 - Training loss: 0.048764873296022415: 100%|██████████| 4/4 [00:00<00:00, 16.41it/s]\n",
            "Epoch 20 - Training loss: 0.04864019155502319: 100%|██████████| 4/4 [00:00<00:00,  7.64it/s] \n",
            "Epoch 20 - Training loss: 0.04839980136603117: 100%|██████████| 4/4 [00:00<00:00, 16.26it/s] \n",
            "Epoch 21 - Training loss: 0.04829607252031565: 100%|██████████| 4/4 [00:00<00:00,  7.76it/s]\n",
            "Epoch 21 - Training loss: 0.04808329790830612: 100%|██████████| 4/4 [00:00<00:00, 16.96it/s]\n",
            "Epoch 22 - Training loss: 0.04799871612340212: 100%|██████████| 4/4 [00:00<00:00,  7.54it/s] \n",
            "Epoch 22 - Training loss: 0.04781079478561878: 100%|██████████| 4/4 [00:00<00:00, 17.02it/s] \n",
            "Epoch 23 - Training loss: 0.047743541188538074: 100%|██████████| 4/4 [00:00<00:00,  7.54it/s]\n",
            "Epoch 23 - Training loss: 0.04757778812199831: 100%|██████████| 4/4 [00:00<00:00, 17.42it/s] \n",
            "Epoch 24 - Training loss: 0.0475260391831398: 100%|██████████| 4/4 [00:00<00:00,  7.50it/s] \n",
            "Epoch 24 - Training loss: 0.04737985972315073: 100%|██████████| 4/4 [00:00<00:00, 16.80it/s]\n",
            "Epoch 25 - Training loss: 0.04734195489436388: 100%|██████████| 4/4 [00:00<00:00,  7.75it/s]\n",
            "Epoch 25 - Training loss: 0.04721288289874792: 100%|██████████| 4/4 [00:00<00:00, 15.74it/s] \n",
            "Epoch 26 - Training loss: 0.047187154181301594: 100%|██████████| 4/4 [00:00<00:00,  7.64it/s]\n",
            "Epoch 26 - Training loss: 0.047072842717170715: 100%|██████████| 4/4 [00:00<00:00, 17.12it/s]\n",
            "Epoch 27 - Training loss: 0.04705778323113918: 100%|██████████| 4/4 [00:00<00:00,  7.69it/s]\n",
            "Epoch 27 - Training loss: 0.0469560706987977: 100%|██████████| 4/4 [00:00<00:00, 16.62it/s] \n",
            "Epoch 28 - Training loss: 0.046950262039899826: 100%|██████████| 4/4 [00:00<00:00,  7.57it/s]\n",
            "Epoch 28 - Training loss: 0.04685914982110262: 100%|██████████| 4/4 [00:00<00:00, 16.91it/s]\n",
            "Epoch 29 - Training loss: 0.04686126206070185: 100%|██████████| 4/4 [00:00<00:00,  7.66it/s]\n",
            "Epoch 29 - Training loss: 0.04677894338965416: 100%|██████████| 4/4 [00:00<00:00, 15.35it/s]\n",
            "Epoch 30 - Training loss: 0.046787737868726254: 100%|██████████| 4/4 [00:00<00:00,  7.82it/s]\n",
            "Epoch 30 - Training loss: 0.046712545678019524: 100%|██████████| 4/4 [00:00<00:00, 16.69it/s]\n",
            "Epoch 31 - Training loss: 0.046726969070732594: 100%|██████████| 4/4 [00:00<00:00,  7.71it/s]\n",
            "Epoch 31 - Training loss: 0.04665749054402113: 100%|██████████| 4/4 [00:00<00:00, 17.37it/s] \n",
            "Epoch 32 - Training loss: 0.046676548197865486: 100%|██████████| 4/4 [00:00<00:00,  7.26it/s]\n",
            "Epoch 32 - Training loss: 0.046611503697931767: 100%|██████████| 4/4 [00:00<00:00, 16.35it/s]\n",
            "Epoch 33 - Training loss: 0.0466343080624938: 100%|██████████| 4/4 [00:00<00:00,  7.60it/s]  \n",
            "Epoch 33 - Training loss: 0.04657258838415146: 100%|██████████| 4/4 [00:00<00:00, 15.97it/s] \n",
            "Epoch 34 - Training loss: 0.04659839253872633: 100%|██████████| 4/4 [00:00<00:00,  7.91it/s]\n",
            "Epoch 34 - Training loss: 0.04653909336775541: 100%|██████████| 4/4 [00:00<00:00, 15.30it/s] \n",
            "Epoch 35 - Training loss: 0.046567244455218315: 100%|██████████| 4/4 [00:00<00:00,  7.60it/s]\n",
            "Epoch 35 - Training loss: 0.04650960024446249: 100%|██████████| 4/4 [00:00<00:00, 16.91it/s]\n",
            "Epoch 36 - Training loss: 0.046539537608623505: 100%|██████████| 4/4 [00:00<00:00,  7.74it/s]\n",
            "Epoch 36 - Training loss: 0.04648293275386095: 100%|██████████| 4/4 [00:00<00:00, 17.23it/s]\n",
            "Epoch 37 - Training loss: 0.046514189802110195: 100%|██████████| 4/4 [00:00<00:00,  7.62it/s]\n",
            "Epoch 37 - Training loss: 0.04645817540585995: 100%|██████████| 4/4 [00:00<00:00, 15.26it/s] \n",
            "Epoch 38 - Training loss: 0.04649042524397373: 100%|██████████| 4/4 [00:00<00:00,  7.68it/s]\n",
            "Epoch 38 - Training loss: 0.04643469396978617: 100%|██████████| 4/4 [00:00<00:00, 15.44it/s]\n",
            "Epoch 39 - Training loss: 0.04646766372025013: 100%|██████████| 4/4 [00:00<00:00,  7.72it/s]\n",
            "Epoch 39 - Training loss: 0.046411994844675064: 100%|██████████| 4/4 [00:00<00:00, 15.95it/s]\n",
            "Epoch 40 - Training loss: 0.04644549638032913: 100%|██████████| 4/4 [00:00<00:00,  7.49it/s] \n",
            "Epoch 40 - Training loss: 0.046389770694077015: 100%|██████████| 4/4 [00:00<00:00, 17.19it/s]\n",
            "Epoch 41 - Training loss: 0.04642367083579302: 100%|██████████| 4/4 [00:00<00:00,  7.65it/s]\n",
            "Epoch 41 - Training loss: 0.04636782128363848: 100%|██████████| 4/4 [00:00<00:00, 16.78it/s] \n",
            "Epoch 42 - Training loss: 0.046402026899158955: 100%|██████████| 4/4 [00:00<00:00,  7.62it/s]\n",
            "Epoch 42 - Training loss: 0.04634602926671505: 100%|██████████| 4/4 [00:00<00:00, 15.30it/s]\n",
            "Epoch 43 - Training loss: 0.04638047143816948: 100%|██████████| 4/4 [00:00<00:00,  7.76it/s]\n",
            "Epoch 43 - Training loss: 0.04632432386279106: 100%|██████████| 4/4 [00:00<00:00, 17.03it/s] \n",
            "Epoch 44 - Training loss: 0.04635895695537329: 100%|██████████| 4/4 [00:00<00:00,  7.61it/s]\n",
            "Epoch 44 - Training loss: 0.04630266781896353: 100%|██████████| 4/4 [00:00<00:00, 17.02it/s]\n",
            "Epoch 45 - Training loss: 0.04633746203035116: 100%|██████████| 4/4 [00:00<00:00,  7.53it/s] \n",
            "Epoch 45 - Training loss: 0.04628105368465185: 100%|██████████| 4/4 [00:00<00:00, 16.18it/s]\n",
            "Epoch 46 - Training loss: 0.04631598945707083: 100%|██████████| 4/4 [00:00<00:00,  7.47it/s] \n",
            "Epoch 46 - Training loss: 0.04625948704779148: 100%|██████████| 4/4 [00:00<00:00, 16.48it/s]\n",
            "Epoch 47 - Training loss: 0.04629454109817743: 100%|██████████| 4/4 [00:00<00:00,  7.91it/s] \n",
            "Epoch 47 - Training loss: 0.0462379464879632: 100%|██████████| 4/4 [00:00<00:00, 15.41it/s]  \n",
            "Epoch 48 - Training loss: 0.04627312533557415: 100%|██████████| 4/4 [00:00<00:00,  7.26it/s]\n",
            "Epoch 48 - Training loss: 0.046216461807489395: 100%|██████████| 4/4 [00:00<00:00, 16.29it/s]\n",
            "Epoch 49 - Training loss: 0.0462517524138093: 100%|██████████| 4/4 [00:00<00:00,  7.60it/s]  \n",
            "Epoch 49 - Training loss: 0.046195026487112045: 100%|██████████| 4/4 [00:00<00:00, 15.93it/s]\n",
            "Epoch 50 - Training loss: 0.046230435371398926: 100%|██████████| 4/4 [00:00<00:00,  7.44it/s]\n",
            "Epoch 50 - Training loss: 0.04617366939783096: 100%|██████████| 4/4 [00:00<00:00, 16.97it/s] \n",
            "Epoch 51 - Training loss: 0.04620919469743967: 100%|██████████| 4/4 [00:00<00:00,  7.50it/s] \n",
            "Epoch 51 - Training loss: 0.04615237936377525: 100%|██████████| 4/4 [00:00<00:00, 16.50it/s]\n",
            "Epoch 52 - Training loss: 0.04618803225457668: 100%|██████████| 4/4 [00:00<00:00,  7.70it/s]\n",
            "Epoch 52 - Training loss: 0.04613117128610611: 100%|██████████| 4/4 [00:00<00:00, 16.25it/s]\n",
            "Epoch 53 - Training loss: 0.046166944317519665: 100%|██████████| 4/4 [00:00<00:00,  7.73it/s]\n",
            "Epoch 53 - Training loss: 0.0461100684478879: 100%|██████████| 4/4 [00:00<00:00, 15.47it/s] \n",
            "Epoch 54 - Training loss: 0.046145953238010406: 100%|██████████| 4/4 [00:00<00:00,  7.54it/s]\n",
            "Epoch 54 - Training loss: 0.04608907178044319: 100%|██████████| 4/4 [00:00<00:00, 17.27it/s]\n",
            "Epoch 55 - Training loss: 0.046125058084726334: 100%|██████████| 4/4 [00:00<00:00,  7.77it/s]\n",
            "Epoch 55 - Training loss: 0.04606814309954643: 100%|██████████| 4/4 [00:00<00:00, 16.62it/s]\n",
            "Epoch 56 - Training loss: 0.046104258857667446: 100%|██████████| 4/4 [00:00<00:00,  7.76it/s]\n",
            "Epoch 56 - Training loss: 0.04604731500148773: 100%|██████████| 4/4 [00:00<00:00, 16.33it/s] \n",
            "Epoch 57 - Training loss: 0.04608356952667236: 100%|██████████| 4/4 [00:00<00:00,  7.81it/s]\n",
            "Epoch 57 - Training loss: 0.04602660797536373: 100%|██████████| 4/4 [00:00<00:00, 16.59it/s]\n",
            "Epoch 58 - Training loss: 0.04606297425925732: 100%|██████████| 4/4 [00:00<00:00,  7.61it/s]\n",
            "Epoch 58 - Training loss: 0.046006002463400364: 100%|██████████| 4/4 [00:00<00:00, 17.19it/s]\n",
            "Epoch 59 - Training loss: 0.0460424954071641: 100%|██████████| 4/4 [00:00<00:00,  7.67it/s]\n",
            "Epoch 59 - Training loss: 0.04598550405353308: 100%|██████████| 4/4 [00:00<00:00, 15.91it/s] \n",
            "Epoch 60 - Training loss: 0.04602211061865091: 100%|██████████| 4/4 [00:00<00:00,  7.52it/s]\n",
            "Epoch 60 - Training loss: 0.04596511460840702: 100%|██████████| 4/4 [00:00<00:00, 15.56it/s]\n",
            "Epoch 61 - Training loss: 0.04600183852016926: 100%|██████████| 4/4 [00:00<00:00,  7.63it/s]\n",
            "Epoch 61 - Training loss: 0.045944822020828724: 100%|██████████| 4/4 [00:00<00:00, 16.40it/s]\n",
            "Epoch 62 - Training loss: 0.04598167724907398: 100%|██████████| 4/4 [00:00<00:00,  7.50it/s]\n",
            "Epoch 62 - Training loss: 0.045924633741378784: 100%|██████████| 4/4 [00:00<00:00, 16.05it/s]\n",
            "Epoch 63 - Training loss: 0.04596161376684904: 100%|██████████| 4/4 [00:00<00:00,  7.72it/s]\n",
            "Epoch 63 - Training loss: 0.04590456746518612: 100%|██████████| 4/4 [00:00<00:00, 16.17it/s]\n",
            "Epoch 64 - Training loss: 0.045941656455397606: 100%|██████████| 4/4 [00:00<00:00,  7.81it/s]\n",
            "Epoch 64 - Training loss: 0.04588458500802517: 100%|██████████| 4/4 [00:00<00:00, 16.96it/s]\n",
            "Epoch 65 - Training loss: 0.04592180158942938: 100%|██████████| 4/4 [00:00<00:00,  7.54it/s]\n",
            "Epoch 65 - Training loss: 0.04586472921073437: 100%|██████████| 4/4 [00:00<00:00, 16.62it/s] \n",
            "Epoch 66 - Training loss: 0.04590206127613783: 100%|██████████| 4/4 [00:00<00:00,  7.11it/s]\n",
            "Epoch 66 - Training loss: 0.04584497772157192: 100%|██████████| 4/4 [00:00<00:00, 16.97it/s]\n",
            "Epoch 67 - Training loss: 0.04588241223245859: 100%|██████████| 4/4 [00:00<00:00,  7.84it/s]\n",
            "Epoch 67 - Training loss: 0.04582530539482832: 100%|██████████| 4/4 [00:00<00:00, 16.71it/s] \n",
            "Epoch 68 - Training loss: 0.04586286284029484: 100%|██████████| 4/4 [00:00<00:00,  7.78it/s]\n",
            "Epoch 68 - Training loss: 0.04580574482679367: 100%|██████████| 4/4 [00:00<00:00, 15.50it/s]\n",
            "Epoch 69 - Training loss: 0.04584341682493687: 100%|██████████| 4/4 [00:00<00:00,  7.73it/s]\n",
            "Epoch 69 - Training loss: 0.04578625876456499: 100%|██████████| 4/4 [00:00<00:00, 15.68it/s]\n",
            "Epoch 70 - Training loss: 0.04582407046109438: 100%|██████████| 4/4 [00:00<00:00,  7.50it/s]\n",
            "Epoch 70 - Training loss: 0.045766896568238735: 100%|██████████| 4/4 [00:00<00:00, 16.76it/s]\n",
            "Epoch 71 - Training loss: 0.045804817229509354: 100%|██████████| 4/4 [00:00<00:00,  7.14it/s]\n",
            "Epoch 71 - Training loss: 0.045747626572847366: 100%|██████████| 4/4 [00:00<00:00, 15.69it/s]\n",
            "Epoch 72 - Training loss: 0.045785666443407536: 100%|██████████| 4/4 [00:00<00:00,  7.71it/s]\n",
            "Epoch 72 - Training loss: 0.04572845809161663: 100%|██████████| 4/4 [00:00<00:00, 16.31it/s]\n",
            "Epoch 73 - Training loss: 0.04576659854501486: 100%|██████████| 4/4 [00:00<00:00,  7.60it/s]\n",
            "Epoch 73 - Training loss: 0.04570937994867563: 100%|██████████| 4/4 [00:00<00:00, 17.02it/s]\n",
            "Epoch 74 - Training loss: 0.04574762284755707: 100%|██████████| 4/4 [00:00<00:00,  7.53it/s] \n",
            "Epoch 74 - Training loss: 0.04569038096815348: 100%|██████████| 4/4 [00:00<00:00, 16.77it/s] \n",
            "Epoch 75 - Training loss: 0.04572874214500189: 100%|██████████| 4/4 [00:00<00:00,  7.75it/s]\n",
            "Epoch 75 - Training loss: 0.04567147325724363: 100%|██████████| 4/4 [00:00<00:00, 15.32it/s]\n",
            "Epoch 76 - Training loss: 0.045709953643381596: 100%|██████████| 4/4 [00:00<00:00,  7.40it/s]\n",
            "Epoch 76 - Training loss: 0.045652675442397594: 100%|██████████| 4/4 [00:00<00:00, 15.25it/s]\n",
            "Epoch 77 - Training loss: 0.04569123778492212: 100%|██████████| 4/4 [00:00<00:00,  7.49it/s]\n",
            "Epoch 77 - Training loss: 0.045633927918970585: 100%|██████████| 4/4 [00:00<00:00, 16.92it/s]\n",
            "Epoch 78 - Training loss: 0.04567261692136526: 100%|██████████| 4/4 [00:00<00:00,  7.26it/s]\n",
            "Epoch 78 - Training loss: 0.04561526980251074: 100%|██████████| 4/4 [00:00<00:00, 16.72it/s] \n",
            "Epoch 79 - Training loss: 0.04565408267080784: 100%|██████████| 4/4 [00:00<00:00,  7.62it/s]\n",
            "Epoch 79 - Training loss: 0.0455967178568244: 100%|██████████| 4/4 [00:00<00:00, 16.34it/s]  \n",
            "Epoch 80 - Training loss: 0.04563561640679836: 100%|██████████| 4/4 [00:00<00:00,  7.83it/s]\n",
            "Epoch 80 - Training loss: 0.04557822644710541: 100%|██████████| 4/4 [00:00<00:00, 16.37it/s]\n",
            "Epoch 81 - Training loss: 0.04561723582446575: 100%|██████████| 4/4 [00:00<00:00,  7.78it/s] \n",
            "Epoch 81 - Training loss: 0.04555982444435358: 100%|██████████| 4/4 [00:00<00:00, 17.19it/s] \n",
            "Epoch 82 - Training loss: 0.04559893626719713: 100%|██████████| 4/4 [00:00<00:00,  7.47it/s]\n",
            "Epoch 82 - Training loss: 0.045541501604020596: 100%|██████████| 4/4 [00:00<00:00, 16.50it/s]\n",
            "Epoch 83 - Training loss: 0.04558070655912161: 100%|██████████| 4/4 [00:00<00:00,  7.77it/s]\n",
            "Epoch 83 - Training loss: 0.04552324675023556: 100%|██████████| 4/4 [00:00<00:00, 15.95it/s]\n",
            "Epoch 84 - Training loss: 0.04556255508214235: 100%|██████████| 4/4 [00:00<00:00,  7.70it/s]\n",
            "Epoch 84 - Training loss: 0.045505073852837086: 100%|██████████| 4/4 [00:00<00:00, 15.80it/s]\n",
            "Epoch 85 - Training loss: 0.04554447252303362: 100%|██████████| 4/4 [00:00<00:00,  7.88it/s]\n",
            "Epoch 85 - Training loss: 0.04548695869743824: 100%|██████████| 4/4 [00:00<00:00, 17.00it/s]\n",
            "Epoch 86 - Training loss: 0.045526466332376: 100%|██████████| 4/4 [00:00<00:00,  7.64it/s]  \n",
            "Epoch 86 - Training loss: 0.04546893481165171: 100%|██████████| 4/4 [00:00<00:00, 14.85it/s]\n",
            "Epoch 87 - Training loss: 0.04550852347165346: 100%|██████████| 4/4 [00:00<00:00,  6.97it/s]\n",
            "Epoch 87 - Training loss: 0.04545095469802618: 100%|██████████| 4/4 [00:00<00:00, 16.58it/s] \n",
            "Epoch 88 - Training loss: 0.045490650460124016: 100%|██████████| 4/4 [00:00<00:00,  7.72it/s]\n",
            "Epoch 88 - Training loss: 0.04543305095285177: 100%|██████████| 4/4 [00:00<00:00, 15.88it/s]\n",
            "Epoch 89 - Training loss: 0.04547284357249737: 100%|██████████| 4/4 [00:00<00:00,  7.77it/s] \n",
            "Epoch 89 - Training loss: 0.04541518911719322: 100%|██████████| 4/4 [00:00<00:00, 16.31it/s] \n",
            "Epoch 90 - Training loss: 0.04545510746538639: 100%|██████████| 4/4 [00:00<00:00,  7.74it/s]\n",
            "Epoch 90 - Training loss: 0.04539741761982441: 100%|██████████| 4/4 [00:00<00:00, 16.55it/s]\n",
            "Epoch 91 - Training loss: 0.04543742164969444: 100%|██████████| 4/4 [00:00<00:00,  7.33it/s] \n",
            "Epoch 91 - Training loss: 0.045379708521068096: 100%|██████████| 4/4 [00:00<00:00, 17.02it/s]\n",
            "Epoch 92 - Training loss: 0.04541980940848589: 100%|██████████| 4/4 [00:00<00:00,  7.62it/s]\n",
            "Epoch 92 - Training loss: 0.0453620720654726: 100%|██████████| 4/4 [00:00<00:00, 15.89it/s]  \n",
            "Epoch 93 - Training loss: 0.04540224093943834: 100%|██████████| 4/4 [00:00<00:00,  7.53it/s]\n",
            "Epoch 93 - Training loss: 0.04534447006881237: 100%|██████████| 4/4 [00:00<00:00, 16.38it/s] \n",
            "Epoch 94 - Training loss: 0.04538473393768072: 100%|██████████| 4/4 [00:00<00:00,  7.75it/s]\n",
            "Epoch 94 - Training loss: 0.04532691650092602: 100%|██████████| 4/4 [00:00<00:00, 17.01it/s]\n",
            "Epoch 95 - Training loss: 0.045367288403213024: 100%|██████████| 4/4 [00:00<00:00,  7.79it/s]\n",
            "Epoch 95 - Training loss: 0.04530944861471653: 100%|██████████| 4/4 [00:00<00:00, 14.96it/s] \n",
            "Epoch 96 - Training loss: 0.045349893160164356: 100%|██████████| 4/4 [00:00<00:00,  7.54it/s]\n",
            "Epoch 96 - Training loss: 0.04529201053082943: 100%|██████████| 4/4 [00:00<00:00, 16.96it/s]\n",
            "Epoch 97 - Training loss: 0.04533255472779274: 100%|██████████| 4/4 [00:00<00:00,  7.70it/s] \n",
            "Epoch 97 - Training loss: 0.04527461901307106: 100%|██████████| 4/4 [00:00<00:00, 17.12it/s]\n",
            "Epoch 98 - Training loss: 0.045315262861549854: 100%|██████████| 4/4 [00:00<00:00,  7.68it/s]\n",
            "Epoch 98 - Training loss: 0.04525728430598974: 100%|██████████| 4/4 [00:00<00:00, 14.63it/s]\n",
            "Epoch 99 - Training loss: 0.045298025012016296: 100%|██████████| 4/4 [00:00<00:00,  7.65it/s]\n",
            "Epoch 99 - Training loss: 0.045240018516778946: 100%|██████████| 4/4 [00:00<00:00, 16.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EghVuqY7anjQ",
        "colab_type": "text"
      },
      "source": [
        "# Capacity Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78G0ksfl2-SF",
        "colab_type": "code",
        "outputId": "bd521ed8-2198-43bd-9593-bf399a7b4fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nengo\n",
        "class CapacityExperiment():\n",
        "    n_gpus = 1\n",
        "    data_length_factor = 2.5\n",
        "    batch_size = 50\n",
        "    validation_split = 0.2\n",
        "\n",
        "    def data_sets(self, T):\n",
        "        n_batches = 200\n",
        "        theta = 1.0\n",
        "        dt = theta / T\n",
        "        freq = 10\n",
        "        n_outputs = 5\n",
        "        length = int(self.data_length_factor / dt + 1e-7)\n",
        "        test_split = 0.5\n",
        "        seed = 0\n",
        "        \n",
        "        rng = np.random.RandomState(seed=seed)\n",
        "        process = nengo.processes.WhiteSignal(length * dt, high=freq, y0=0)\n",
        "\n",
        "        # t = process.ntrange(length, dt=dt)\n",
        "        X = np.empty((n_batches, length, 1))  # 1 input\n",
        "        Y = np.zeros((n_batches, length, n_outputs))\n",
        "\n",
        "        delay = int(theta / dt + 1e-7)\n",
        "        assert T == delay\n",
        "        s = np.linspace(0, delay, n_outputs, dtype=int)\n",
        "\n",
        "        def _generate(x, y):\n",
        "            x[...] = process.run_steps(length, dt=dt, rng=rng)\n",
        "            x[...] /= np.max(np.abs(x))\n",
        "            for i in range(n_outputs):\n",
        "                y[s[i]:, i] = x[:-s[i], 0] if s[i] > 0 else x[:, 0]\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            _generate(X[i, :], Y[i, :])\n",
        "\n",
        "        cutoff = int(test_split*n_batches)\n",
        "\n",
        "        train_X = X[:cutoff]\n",
        "        train_Y = Y[:cutoff]\n",
        "\n",
        "        test_X = X[cutoff:]\n",
        "        test_Y = Y[cutoff:]\n",
        "\n",
        "        return s, (train_X, train_Y), (test_X, test_Y)\n",
        "\n",
        "\n",
        "\n",
        "ce = CapacityExperiment()\n",
        "s, (train_X, train_Y), (test_X, test_Y) = ce.data_sets(100)\n",
        "print(train_X.shape, train_Y.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 250, 1) (100, 250, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}